{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e696bff6",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7df7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5c0a7",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4ad9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded: (148670, 34)\n",
      "\n",
      "Dataset shape: 148,670 rows × 34 columns\n",
      "\n",
      "First rows:\n",
      "      ID  year loan_limit             Gender approv_in_adv loan_type  \\\n",
      "0  24890  2019         cf  Sex Not Available         nopre     type1   \n",
      "1  24891  2019         cf               Male         nopre     type2   \n",
      "2  24892  2019         cf               Male           pre     type1   \n",
      "3  24893  2019         cf               Male         nopre     type1   \n",
      "4  24894  2019         cf              Joint           pre     type1   \n",
      "\n",
      "  loan_purpose Credit_Worthiness open_credit business_or_commercial  ...  \\\n",
      "0           p1                l1        nopc                  nob/c  ...   \n",
      "1           p1                l1        nopc                    b/c  ...   \n",
      "2           p1                l1        nopc                  nob/c  ...   \n",
      "3           p4                l1        nopc                  nob/c  ...   \n",
      "4           p1                l1        nopc                  nob/c  ...   \n",
      "\n",
      "   credit_type  Credit_Score  co-applicant_credit_type    age  \\\n",
      "0          EXP           758                       CIB  25-34   \n",
      "1         EQUI           552                       EXP  55-64   \n",
      "2          EXP           834                       CIB  35-44   \n",
      "3          EXP           587                       CIB  45-54   \n",
      "4         CRIF           602                       EXP  25-34   \n",
      "\n",
      "   submission_of_application        LTV Region Security_Type  Status dtir1  \n",
      "0                    to_inst  98.728814  south        direct       1  45.0  \n",
      "1                    to_inst        NaN  North        direct       1   NaN  \n",
      "2                    to_inst  80.019685  south        direct       0  46.0  \n",
      "3                   not_inst  69.376900  North        direct       0  42.0  \n",
      "4                   not_inst  91.886544  North        direct       0  39.0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\user\\Documents\\dev\\selastone_loan_default\\archive\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(csv_path, 'Loan_Default.csv'))\n",
    "print(f\"✓ Data loaded: {df.shape}\")\n",
    "print(f\"\\nDataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6969610",
   "metadata": {},
   "source": [
    "## EXPLORE & CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27afb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA EXPLORATION & CLEANING\n",
      "======================================================================\n",
      "\n",
      "Target Variable (Status):\n",
      "Status\n",
      "0    112031\n",
      "1     36639\n",
      "Name: count, dtype: int64\n",
      "Default Rate: 24.64%\n",
      "\n",
      "Missing Values Summary:\n",
      "Upfront_charges              26.664425\n",
      "Interest_rate_spread         24.644515\n",
      "rate_of_interest             24.509989\n",
      "dtir1                        16.224524\n",
      "LTV                          10.155378\n",
      "property_value               10.155378\n",
      "income                        6.154571\n",
      "loan_limit                    2.249277\n",
      "approv_in_adv                 0.610749\n",
      "submission_of_application     0.134526\n",
      "age                           0.134526\n",
      "loan_purpose                  0.090133\n",
      "Neg_ammortization             0.081388\n",
      "term                          0.027578\n",
      "dtype: float64\n",
      "\n",
      "Dropping 0 columns with >40% missing:\n",
      "[]\n",
      "\n",
      "Dataset shape after cleaning: (148670, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA EXPLORATION & CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check target variable\n",
    "print(f\"\\nTarget Variable (Status):\")\n",
    "print(df['Status'].value_counts())\n",
    "print(f\"Default Rate: {df['Status'].mean():.2%}\")\n",
    "\n",
    "# Remove ID column (not a feature)\n",
    "df = df.drop(['ID'], axis=1)\n",
    "\n",
    "# Missing values\n",
    "print(f\"\\nMissing Values Summary:\")\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0].head(15))\n",
    "\n",
    "# Drop columns with >40% missing\n",
    "drop_cols = missing_pct[missing_pct > 40].index.tolist()\n",
    "print(f\"\\nDropping {len(drop_cols)} columns with >40% missing:\")\n",
    "print(drop_cols)\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "print(f\"\\nDataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea54a49",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0122708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING\n",
      "======================================================================\n",
      "Numeric columns (11): ['year', 'loan_amount', 'rate_of_interest', 'Interest_rate_spread', 'Upfront_charges', 'term', 'property_value', 'income', 'Credit_Score', 'LTV', 'dtir1']\n",
      "\n",
      "Categorical columns (21): ['loan_limit', 'Gender', 'approv_in_adv', 'loan_type', 'loan_purpose', 'Credit_Worthiness', 'open_credit', 'business_or_commercial', 'Neg_ammortization', 'interest_only', 'lump_sum_payment', 'construction_type', 'occupancy_type', 'Secured_by', 'total_units', 'credit_type', 'co-applicant_credit_type', 'age', 'submission_of_application', 'Region', 'Security_Type']\n",
      "\n",
      "✓ Missing values filled\n",
      "✓ Created 3 derived features\n",
      "✓ Total numeric features: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from features\n",
    "if 'Status' in numeric_cols:\n",
    "    numeric_cols.remove('Status')\n",
    "\n",
    "print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill missing categorical values with mode\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"\\n✓ Missing values filled\")\n",
    "\n",
    "# Create derived features\n",
    "df['loan_to_income'] = df['loan_amount'] / (df['income'] + 1)\n",
    "df['loan_to_property'] = df['loan_amount'] / (df['property_value'] + 1)\n",
    "df['credit_to_income'] = df['Credit_Score'] / (df['income'] + 1)\n",
    "\n",
    "# Add new features to numeric columns\n",
    "new_features = ['loan_to_income', 'loan_to_property', 'credit_to_income']\n",
    "numeric_cols.extend(new_features)\n",
    "\n",
    "print(f\"✓ Created {len(new_features)} derived features\")\n",
    "print(f\"✓ Total numeric features: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd9dea",
   "metadata": {},
   "source": [
    "## PREPARE FEATURES FOR MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6850f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPARE FEATURES\n",
      "======================================================================\n",
      "Features shape: (148670, 35)\n",
      "Target shape: (148670,)\n",
      "Target distribution:\n",
      "  0 (No Default): 112,031\n",
      "  1 (Default): 36,639\n",
      "✓ Encoded 21 categorical columns\n",
      "✓ Handled outliers\n",
      "\n",
      "Final features: ['year', 'loan_amount', 'rate_of_interest', 'Interest_rate_spread', 'Upfront_charges', 'term', 'property_value', 'income', 'Credit_Score', 'LTV', 'dtir1', 'loan_to_income', 'loan_to_property', 'credit_to_income', 'loan_limit', 'Gender', 'approv_in_adv', 'loan_type', 'loan_purpose', 'Credit_Worthiness', 'open_credit', 'business_or_commercial', 'Neg_ammortization', 'interest_only', 'lump_sum_payment', 'construction_type', 'occupancy_type', 'Secured_by', 'total_units', 'credit_type', 'co-applicant_credit_type', 'age', 'submission_of_application', 'Region', 'Security_Type']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate X and y\n",
    "X = df[numeric_cols + categorical_cols].copy()\n",
    "y = df['Status'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(f\"  0 (No Default): {(y == 0).sum():,}\")\n",
    "print(f\"  1 (Default): {(y == 1).sum():,}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"✓ Encoded {len(categorical_cols)} categorical columns\")\n",
    "\n",
    "# Handle outliers (clip at 1st and 99th percentiles)\n",
    "for col in numeric_cols:\n",
    "    q1 = X[col].quantile(0.01)\n",
    "    q99 = X[col].quantile(0.99)\n",
    "    X[col] = X[col].clip(q1, q99)\n",
    "\n",
    "print(f\"✓ Handled outliers\")\n",
    "print(f\"\\nFinal features: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5d842",
   "metadata": {},
   "source": [
    "## TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd91cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAIN-TEST SPLIT\n",
      "======================================================================\n",
      "Training set: (118936, 35)\n",
      "  Default rate: 24.64%\n",
      "\n",
      "Test set: (29734, 35)\n",
      "  Default rate: 24.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"  Default rate: {y_train.mean():.2%}\")\n",
    "print(f\"\\nTest set: {X_test.shape}\")\n",
    "print(f\"  Default rate: {y_test.mean():.2%}\")\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9191f5",
   "metadata": {},
   "source": [
    "## SCALE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec592c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCALE FEATURES\n",
      "======================================================================\n",
      "✓ Features scaled (mean=0, std=1)\n",
      "Training set shape: (118936, 35)\n",
      "Test set shape: (29734, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Features scaled (mean=0, std=1)\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e48df",
   "metadata": {},
   "source": [
    "## HANDLE CLASS IMBALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edff9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HANDLE CLASS IMBALANCE (SMOTE)\n",
      "======================================================================\n",
      "Before SMOTE:\n",
      "  Shape: (118936, 35)\n",
      "  Defaults: 29,311 (24.64%)\n",
      "\n",
      "After SMOTE:\n",
      "  Shape: (161325, 35)\n",
      "  Defaults: 71,700 (44.44%)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLE CLASS IMBALANCE (SMOTE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "smote = SMOTE(\n",
    "    k_neighbors=3,        # Default is 5, try 3 for sparse regions\n",
    "    sampling_strategy=0.8, # 0.8 instead of 1.0 (keep some imbalance)\n",
    "    random_state=42\n",
    ")\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE:\")\n",
    "print(f\"  Shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Defaults: {y_train.sum():,} ({y_train.mean():.2%})\")\n",
    "\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"  Shape: {X_train_balanced.shape}\")\n",
    "print(f\"  Defaults: {y_train_balanced.sum():,} ({y_train_balanced.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea349d7",
   "metadata": {},
   "source": [
    "## BASELINE MODEL (YOUR CURRENT MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503c1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE LIGHTGBM MODEL (YOUR CURRENT PARAMS)\n",
      "======================================================================\n",
      "Training baseline with your current parameters...\n",
      "  n_estimators: 500\n",
      "  max_depth: 4\n",
      "  learning_rate: 0.03\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.7\n",
      "  min_child_samples: 20\n",
      "  reg_lambda: 1.0\n",
      "  reg_alpha: 0.5\n",
      "  scale_pos_weight: 3.06\n",
      "\n",
      "✓ Baseline model trained\n",
      "\n",
      "Baseline Results:\n",
      "  AUC-ROC:  0.4144\n",
      "  F1-Score: 0.0000\n",
      "  Accuracy: 0.7534\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE LIGHTGBM MODEL (YOUR CURRENT PARAMS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "baseline_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_samples=20,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(f\"Training baseline with your current parameters...\")\n",
    "print(f\"  n_estimators: 500\")\n",
    "print(f\"  max_depth: 4\")\n",
    "print(f\"  learning_rate: 0.03\")\n",
    "print(f\"  subsample: 0.7\")\n",
    "print(f\"  colsample_bytree: 0.7\")\n",
    "print(f\"  min_child_samples: 20\")\n",
    "print(f\"  reg_lambda: 1.0\")\n",
    "print(f\"  reg_alpha: 0.5\")\n",
    "print(f\"  scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "baseline_lgb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_pred_proba = baseline_lgb.predict_proba(X_test)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, baseline_pred_proba)\n",
    "baseline_f1 = f1_score(y_test, baseline_lgb.predict(X_test))\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_lgb.predict(X_test))\n",
    "\n",
    "print(f\"\\n✓ Baseline model trained\")\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"  AUC-ROC:  {baseline_auc:.4f}\")\n",
    "print(f\"  F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4a76e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
